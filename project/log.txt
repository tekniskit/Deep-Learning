I0111 13:39:34.856765 20026 caffe.cpp:99] Use GPU with device ID 0
I0111 13:39:34.972502 20026 caffe.cpp:107] Starting Optimization
I0111 13:39:34.972568 20026 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.0001
display: 100
max_iter: 20000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 1000
snapshot_prefix: "snapshots/snapshot"
solver_mode: GPU
net: "net/architecture.prototxt"
I0111 13:39:34.972599 20026 solver.cpp:70] Creating training net from net file: net/architecture.prototxt
I0111 13:39:34.972911 20026 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0111 13:39:34.972925 20026 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0111 13:39:34.972991 20026 net.cpp:42] Initializing net from parameters: 
name: "CIFAR10_quick"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "net/mean.binaryproto"
  }
  data_param {
    source: "net/train-db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool1"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "relu3"
  top: "ip1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "relu4"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "relu4"
  top: "ip2"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip2"
  top: "relu5"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "relu5"
  top: "ip3"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0111 13:39:34.973047 20026 layer_factory.hpp:74] Creating layer cifar
I0111 13:39:34.973067 20026 net.cpp:76] Creating Layer cifar
I0111 13:39:34.973072 20026 net.cpp:334] cifar -> data
I0111 13:39:34.973090 20026 net.cpp:334] cifar -> label
I0111 13:39:34.973098 20026 net.cpp:105] Setting up cifar
I0111 13:39:34.973150 20026 db.cpp:34] Opened lmdb net/train-db
I0111 13:39:34.973176 20026 data_layer.cpp:67] output data size: 100,3,32,32
I0111 13:39:34.973183 20026 data_transformer.cpp:22] Loading mean file from: net/mean.binaryproto
I0111 13:39:34.973534 20026 net.cpp:112] Top shape: 100 3 32 32 (307200)
I0111 13:39:34.973541 20026 net.cpp:112] Top shape: 100 1 1 1 (100)
I0111 13:39:34.973546 20026 layer_factory.hpp:74] Creating layer conv1
I0111 13:39:34.973551 20026 net.cpp:76] Creating Layer conv1
I0111 13:39:34.973554 20026 net.cpp:372] conv1 <- data
I0111 13:39:34.973562 20026 net.cpp:334] conv1 -> conv1
I0111 13:39:34.973570 20026 net.cpp:105] Setting up conv1
I0111 13:39:34.973958 20026 net.cpp:112] Top shape: 100 32 32 32 (3276800)
I0111 13:39:34.973973 20026 layer_factory.hpp:74] Creating layer relu1
I0111 13:39:34.973978 20026 net.cpp:76] Creating Layer relu1
I0111 13:39:34.973992 20026 net.cpp:372] relu1 <- conv1
I0111 13:39:34.974000 20026 net.cpp:334] relu1 -> relu1
I0111 13:39:34.974009 20026 net.cpp:105] Setting up relu1
I0111 13:39:34.974015 20026 net.cpp:112] Top shape: 100 32 32 32 (3276800)
I0111 13:39:34.974020 20026 layer_factory.hpp:74] Creating layer pool1
I0111 13:39:34.974025 20026 net.cpp:76] Creating Layer pool1
I0111 13:39:34.974030 20026 net.cpp:372] pool1 <- relu1
I0111 13:39:34.974035 20026 net.cpp:334] pool1 -> pool1
I0111 13:39:34.974046 20026 net.cpp:105] Setting up pool1
I0111 13:39:34.974056 20026 net.cpp:112] Top shape: 100 32 31 31 (3075200)
I0111 13:39:34.974059 20026 layer_factory.hpp:74] Creating layer conv3
I0111 13:39:34.974066 20026 net.cpp:76] Creating Layer conv3
I0111 13:39:34.974067 20026 net.cpp:372] conv3 <- pool1
I0111 13:39:34.974072 20026 net.cpp:334] conv3 -> conv3
I0111 13:39:34.974076 20026 net.cpp:105] Setting up conv3
I0111 13:39:34.974531 20026 net.cpp:112] Top shape: 100 64 31 31 (6150400)
I0111 13:39:34.974539 20026 layer_factory.hpp:74] Creating layer relu3
I0111 13:39:34.974542 20026 net.cpp:76] Creating Layer relu3
I0111 13:39:34.974545 20026 net.cpp:372] relu3 <- conv3
I0111 13:39:34.974547 20026 net.cpp:334] relu3 -> relu3
I0111 13:39:34.974551 20026 net.cpp:105] Setting up relu3
I0111 13:39:34.974553 20026 net.cpp:112] Top shape: 100 64 31 31 (6150400)
I0111 13:39:34.974555 20026 layer_factory.hpp:74] Creating layer ip1
I0111 13:39:34.974560 20026 net.cpp:76] Creating Layer ip1
I0111 13:39:34.974563 20026 net.cpp:372] ip1 <- relu3
I0111 13:39:34.974566 20026 net.cpp:334] ip1 -> ip1
I0111 13:39:34.974572 20026 net.cpp:105] Setting up ip1
I0111 13:39:35.071523 20026 net.cpp:112] Top shape: 100 64 1 1 (6400)
I0111 13:39:35.071552 20026 layer_factory.hpp:74] Creating layer relu4
I0111 13:39:35.071560 20026 net.cpp:76] Creating Layer relu4
I0111 13:39:35.071563 20026 net.cpp:372] relu4 <- ip1
I0111 13:39:35.071579 20026 net.cpp:334] relu4 -> relu4
I0111 13:39:35.071589 20026 net.cpp:105] Setting up relu4
I0111 13:39:35.071594 20026 net.cpp:112] Top shape: 100 64 1 1 (6400)
I0111 13:39:35.071598 20026 layer_factory.hpp:74] Creating layer ip2
I0111 13:39:35.071605 20026 net.cpp:76] Creating Layer ip2
I0111 13:39:35.071609 20026 net.cpp:372] ip2 <- relu4
I0111 13:39:35.071612 20026 net.cpp:334] ip2 -> ip2
I0111 13:39:35.071617 20026 net.cpp:105] Setting up ip2
I0111 13:39:35.071676 20026 net.cpp:112] Top shape: 100 32 1 1 (3200)
I0111 13:39:35.071681 20026 layer_factory.hpp:74] Creating layer relu5
I0111 13:39:35.071686 20026 net.cpp:76] Creating Layer relu5
I0111 13:39:35.071688 20026 net.cpp:372] relu5 <- ip2
I0111 13:39:35.071691 20026 net.cpp:334] relu5 -> relu5
I0111 13:39:35.071696 20026 net.cpp:105] Setting up relu5
I0111 13:39:35.071698 20026 net.cpp:112] Top shape: 100 32 1 1 (3200)
I0111 13:39:35.071701 20026 layer_factory.hpp:74] Creating layer ip3
I0111 13:39:35.071705 20026 net.cpp:76] Creating Layer ip3
I0111 13:39:35.071707 20026 net.cpp:372] ip3 <- relu5
I0111 13:39:35.071710 20026 net.cpp:334] ip3 -> ip3
I0111 13:39:35.071714 20026 net.cpp:105] Setting up ip3
I0111 13:39:35.071727 20026 net.cpp:112] Top shape: 100 10 1 1 (1000)
I0111 13:39:35.071735 20026 layer_factory.hpp:74] Creating layer loss
I0111 13:39:35.071739 20026 net.cpp:76] Creating Layer loss
I0111 13:39:35.071741 20026 net.cpp:372] loss <- ip3
I0111 13:39:35.071744 20026 net.cpp:372] loss <- label
I0111 13:39:35.071749 20026 net.cpp:334] loss -> loss
I0111 13:39:35.071753 20026 net.cpp:105] Setting up loss
I0111 13:39:35.071763 20026 layer_factory.hpp:74] Creating layer loss
I0111 13:39:35.071779 20026 net.cpp:112] Top shape: 1 1 1 1 (1)
I0111 13:39:35.071781 20026 net.cpp:118]     with loss weight 1
I0111 13:39:35.071797 20026 net.cpp:163] loss needs backward computation.
I0111 13:39:35.071800 20026 net.cpp:163] ip3 needs backward computation.
I0111 13:39:35.071802 20026 net.cpp:163] relu5 needs backward computation.
I0111 13:39:35.071805 20026 net.cpp:163] ip2 needs backward computation.
I0111 13:39:35.071806 20026 net.cpp:163] relu4 needs backward computation.
I0111 13:39:35.071820 20026 net.cpp:163] ip1 needs backward computation.
I0111 13:39:35.071823 20026 net.cpp:163] relu3 needs backward computation.
I0111 13:39:35.071826 20026 net.cpp:163] conv3 needs backward computation.
I0111 13:39:35.071828 20026 net.cpp:163] pool1 needs backward computation.
I0111 13:39:35.071830 20026 net.cpp:163] relu1 needs backward computation.
I0111 13:39:35.071832 20026 net.cpp:163] conv1 needs backward computation.
I0111 13:39:35.071835 20026 net.cpp:165] cifar does not need backward computation.
I0111 13:39:35.071837 20026 net.cpp:201] This network produces output loss
I0111 13:39:35.071846 20026 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0111 13:39:35.071851 20026 net.cpp:213] Network initialization done.
I0111 13:39:35.071852 20026 net.cpp:214] Memory required for data: 89028404
I0111 13:39:35.072139 20026 solver.cpp:154] Creating test net (#0) specified by net file: net/architecture.prototxt
I0111 13:39:35.072161 20026 net.cpp:253] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0111 13:39:35.072237 20026 net.cpp:42] Initializing net from parameters: 
name: "CIFAR10_quick"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "net/mean.binaryproto"
  }
  data_param {
    source: "net/test-db"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool1"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "relu3"
  top: "ip1"
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "relu4"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "relu4"
  top: "ip2"
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip2"
  top: "relu5"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "relu5"
  top: "ip3"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0111 13:39:35.072294 20026 layer_factory.hpp:74] Creating layer cifar
I0111 13:39:35.072301 20026 net.cpp:76] Creating Layer cifar
I0111 13:39:35.072304 20026 net.cpp:334] cifar -> data
I0111 13:39:35.072310 20026 net.cpp:334] cifar -> label
I0111 13:39:35.072315 20026 net.cpp:105] Setting up cifar
I0111 13:39:35.072350 20026 db.cpp:34] Opened lmdb net/test-db
I0111 13:39:35.072365 20026 data_layer.cpp:67] output data size: 100,3,32,32
I0111 13:39:35.072370 20026 data_transformer.cpp:22] Loading mean file from: net/mean.binaryproto
I0111 13:39:35.072708 20026 net.cpp:112] Top shape: 100 3 32 32 (307200)
I0111 13:39:35.072716 20026 net.cpp:112] Top shape: 100 1 1 1 (100)
I0111 13:39:35.072732 20026 layer_factory.hpp:74] Creating layer label_cifar_1_split
I0111 13:39:35.072737 20026 net.cpp:76] Creating Layer label_cifar_1_split
I0111 13:39:35.072739 20026 net.cpp:372] label_cifar_1_split <- label
I0111 13:39:35.072744 20026 net.cpp:334] label_cifar_1_split -> label_cifar_1_split_0
I0111 13:39:35.072749 20026 net.cpp:334] label_cifar_1_split -> label_cifar_1_split_1
I0111 13:39:35.072753 20026 net.cpp:105] Setting up label_cifar_1_split
I0111 13:39:35.072757 20026 net.cpp:112] Top shape: 100 1 1 1 (100)
I0111 13:39:35.072760 20026 net.cpp:112] Top shape: 100 1 1 1 (100)
I0111 13:39:35.072762 20026 layer_factory.hpp:74] Creating layer conv1
I0111 13:39:35.072767 20026 net.cpp:76] Creating Layer conv1
I0111 13:39:35.072770 20026 net.cpp:372] conv1 <- data
I0111 13:39:35.072773 20026 net.cpp:334] conv1 -> conv1
I0111 13:39:35.072778 20026 net.cpp:105] Setting up conv1
I0111 13:39:35.072810 20026 net.cpp:112] Top shape: 100 32 32 32 (3276800)
I0111 13:39:35.072818 20026 layer_factory.hpp:74] Creating layer relu1
I0111 13:39:35.072821 20026 net.cpp:76] Creating Layer relu1
I0111 13:39:35.072824 20026 net.cpp:372] relu1 <- conv1
I0111 13:39:35.072829 20026 net.cpp:334] relu1 -> relu1
I0111 13:39:35.072832 20026 net.cpp:105] Setting up relu1
I0111 13:39:35.072835 20026 net.cpp:112] Top shape: 100 32 32 32 (3276800)
I0111 13:39:35.072837 20026 layer_factory.hpp:74] Creating layer pool1
I0111 13:39:35.072841 20026 net.cpp:76] Creating Layer pool1
I0111 13:39:35.072844 20026 net.cpp:372] pool1 <- relu1
I0111 13:39:35.072849 20026 net.cpp:334] pool1 -> pool1
I0111 13:39:35.072852 20026 net.cpp:105] Setting up pool1
I0111 13:39:35.072855 20026 net.cpp:112] Top shape: 100 32 31 31 (3075200)
I0111 13:39:35.072860 20026 layer_factory.hpp:74] Creating layer conv3
I0111 13:39:35.072863 20026 net.cpp:76] Creating Layer conv3
I0111 13:39:35.072865 20026 net.cpp:372] conv3 <- pool1
I0111 13:39:35.072870 20026 net.cpp:334] conv3 -> conv3
I0111 13:39:35.072875 20026 net.cpp:105] Setting up conv3
I0111 13:39:35.073323 20026 net.cpp:112] Top shape: 100 64 31 31 (6150400)
I0111 13:39:35.073328 20026 layer_factory.hpp:74] Creating layer relu3
I0111 13:39:35.073333 20026 net.cpp:76] Creating Layer relu3
I0111 13:39:35.073336 20026 net.cpp:372] relu3 <- conv3
I0111 13:39:35.073339 20026 net.cpp:334] relu3 -> relu3
I0111 13:39:35.073343 20026 net.cpp:105] Setting up relu3
I0111 13:39:35.073348 20026 net.cpp:112] Top shape: 100 64 31 31 (6150400)
I0111 13:39:35.073349 20026 layer_factory.hpp:74] Creating layer ip1
I0111 13:39:35.073354 20026 net.cpp:76] Creating Layer ip1
I0111 13:39:35.073355 20026 net.cpp:372] ip1 <- relu3
I0111 13:39:35.073361 20026 net.cpp:334] ip1 -> ip1
I0111 13:39:35.073366 20026 net.cpp:105] Setting up ip1
I0111 13:39:35.167196 20026 net.cpp:112] Top shape: 100 64 1 1 (6400)
I0111 13:39:35.167227 20026 layer_factory.hpp:74] Creating layer relu4
I0111 13:39:35.167237 20026 net.cpp:76] Creating Layer relu4
I0111 13:39:35.167240 20026 net.cpp:372] relu4 <- ip1
I0111 13:39:35.167245 20026 net.cpp:334] relu4 -> relu4
I0111 13:39:35.167253 20026 net.cpp:105] Setting up relu4
I0111 13:39:35.167255 20026 net.cpp:112] Top shape: 100 64 1 1 (6400)
I0111 13:39:35.167260 20026 layer_factory.hpp:74] Creating layer ip2
I0111 13:39:35.167268 20026 net.cpp:76] Creating Layer ip2
I0111 13:39:35.167271 20026 net.cpp:372] ip2 <- relu4
I0111 13:39:35.167274 20026 net.cpp:334] ip2 -> ip2
I0111 13:39:35.167279 20026 net.cpp:105] Setting up ip2
I0111 13:39:35.167340 20026 net.cpp:112] Top shape: 100 32 1 1 (3200)
I0111 13:39:35.167346 20026 layer_factory.hpp:74] Creating layer relu5
I0111 13:39:35.167348 20026 net.cpp:76] Creating Layer relu5
I0111 13:39:35.167351 20026 net.cpp:372] relu5 <- ip2
I0111 13:39:35.167354 20026 net.cpp:334] relu5 -> relu5
I0111 13:39:35.167357 20026 net.cpp:105] Setting up relu5
I0111 13:39:35.167359 20026 net.cpp:112] Top shape: 100 32 1 1 (3200)
I0111 13:39:35.167362 20026 layer_factory.hpp:74] Creating layer ip3
I0111 13:39:35.167366 20026 net.cpp:76] Creating Layer ip3
I0111 13:39:35.167368 20026 net.cpp:372] ip3 <- relu5
I0111 13:39:35.167384 20026 net.cpp:334] ip3 -> ip3
I0111 13:39:35.167388 20026 net.cpp:105] Setting up ip3
I0111 13:39:35.167402 20026 net.cpp:112] Top shape: 100 10 1 1 (1000)
I0111 13:39:35.167409 20026 layer_factory.hpp:74] Creating layer ip3_ip3_0_split
I0111 13:39:35.167413 20026 net.cpp:76] Creating Layer ip3_ip3_0_split
I0111 13:39:35.167418 20026 net.cpp:372] ip3_ip3_0_split <- ip3
I0111 13:39:35.167420 20026 net.cpp:334] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0111 13:39:35.167425 20026 net.cpp:334] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0111 13:39:35.167429 20026 net.cpp:105] Setting up ip3_ip3_0_split
I0111 13:39:35.167433 20026 net.cpp:112] Top shape: 100 10 1 1 (1000)
I0111 13:39:35.167435 20026 net.cpp:112] Top shape: 100 10 1 1 (1000)
I0111 13:39:35.167438 20026 layer_factory.hpp:74] Creating layer accuracy
I0111 13:39:35.167445 20026 net.cpp:76] Creating Layer accuracy
I0111 13:39:35.167448 20026 net.cpp:372] accuracy <- ip3_ip3_0_split_0
I0111 13:39:35.167451 20026 net.cpp:372] accuracy <- label_cifar_1_split_0
I0111 13:39:35.167454 20026 net.cpp:334] accuracy -> accuracy
I0111 13:39:35.167459 20026 net.cpp:105] Setting up accuracy
I0111 13:39:35.167464 20026 net.cpp:112] Top shape: 1 1 1 1 (1)
I0111 13:39:35.167466 20026 layer_factory.hpp:74] Creating layer loss
I0111 13:39:35.167472 20026 net.cpp:76] Creating Layer loss
I0111 13:39:35.167474 20026 net.cpp:372] loss <- ip3_ip3_0_split_1
I0111 13:39:35.167477 20026 net.cpp:372] loss <- label_cifar_1_split_1
I0111 13:39:35.167481 20026 net.cpp:334] loss -> loss
I0111 13:39:35.167484 20026 net.cpp:105] Setting up loss
I0111 13:39:35.167489 20026 layer_factory.hpp:74] Creating layer loss
I0111 13:39:35.167497 20026 net.cpp:112] Top shape: 1 1 1 1 (1)
I0111 13:39:35.167501 20026 net.cpp:118]     with loss weight 1
I0111 13:39:35.167511 20026 net.cpp:163] loss needs backward computation.
I0111 13:39:35.167515 20026 net.cpp:165] accuracy does not need backward computation.
I0111 13:39:35.167516 20026 net.cpp:163] ip3_ip3_0_split needs backward computation.
I0111 13:39:35.167518 20026 net.cpp:163] ip3 needs backward computation.
I0111 13:39:35.167521 20026 net.cpp:163] relu5 needs backward computation.
I0111 13:39:35.167523 20026 net.cpp:163] ip2 needs backward computation.
I0111 13:39:35.167526 20026 net.cpp:163] relu4 needs backward computation.
I0111 13:39:35.167528 20026 net.cpp:163] ip1 needs backward computation.
I0111 13:39:35.167531 20026 net.cpp:163] relu3 needs backward computation.
I0111 13:39:35.167532 20026 net.cpp:163] conv3 needs backward computation.
I0111 13:39:35.167536 20026 net.cpp:163] pool1 needs backward computation.
I0111 13:39:35.167537 20026 net.cpp:163] relu1 needs backward computation.
I0111 13:39:35.167539 20026 net.cpp:163] conv1 needs backward computation.
I0111 13:39:35.167542 20026 net.cpp:165] label_cifar_1_split does not need backward computation.
I0111 13:39:35.167544 20026 net.cpp:165] cifar does not need backward computation.
I0111 13:39:35.167546 20026 net.cpp:201] This network produces output accuracy
I0111 13:39:35.167549 20026 net.cpp:201] This network produces output loss
I0111 13:39:35.167559 20026 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0111 13:39:35.167564 20026 net.cpp:213] Network initialization done.
I0111 13:39:35.167567 20026 net.cpp:214] Memory required for data: 89037208
I0111 13:39:35.167616 20026 solver.cpp:42] Solver scaffolding done.
I0111 13:39:35.167634 20026 caffe.cpp:112] Resuming from snapshots/snapshot_iter_16000.solverstate
I0111 13:39:35.167637 20026 solver.cpp:222] Solving CIFAR10_quick
I0111 13:39:35.167639 20026 solver.cpp:223] Learning Rate Policy: fixed
I0111 13:39:35.167641 20026 solver.cpp:226] Restoring previous solver status from snapshots/snapshot_iter_16000.solverstate
I0111 13:39:35.208458 20026 solver.cpp:570] SGDSolver: restoring history
I0111 13:39:35.213340 20026 solver.cpp:266] Iteration 16000, Testing net (#0)
I0111 13:39:36.903594 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6373
I0111 13:39:36.903619 20026 solver.cpp:315]     Test net output #1: loss = 1.10552 (* 1 = 1.10552 loss)
I0111 13:39:36.925833 20026 solver.cpp:189] Iteration 16000, loss = 0.740054
I0111 13:39:36.925856 20026 solver.cpp:204]     Train net output #0: loss = 0.740054 (* 1 = 0.740054 loss)
I0111 13:39:36.925863 20026 solver.cpp:470] Iteration 16000, lr = 0.0001
I0111 13:39:45.270539 20026 solver.cpp:189] Iteration 16100, loss = 0.696258
I0111 13:39:45.270563 20026 solver.cpp:204]     Train net output #0: loss = 0.696258 (* 1 = 0.696258 loss)
I0111 13:39:45.270568 20026 solver.cpp:470] Iteration 16100, lr = 0.0001
I0111 13:39:53.956425 20026 solver.cpp:189] Iteration 16200, loss = 0.80975
I0111 13:39:53.956449 20026 solver.cpp:204]     Train net output #0: loss = 0.80975 (* 1 = 0.80975 loss)
I0111 13:39:53.956454 20026 solver.cpp:470] Iteration 16200, lr = 0.0001
I0111 13:40:02.544173 20026 solver.cpp:189] Iteration 16300, loss = 0.674344
I0111 13:40:02.544198 20026 solver.cpp:204]     Train net output #0: loss = 0.674344 (* 1 = 0.674344 loss)
I0111 13:40:02.544201 20026 solver.cpp:470] Iteration 16300, lr = 0.0001
I0111 13:40:11.137810 20026 solver.cpp:189] Iteration 16400, loss = 0.539237
I0111 13:40:11.137856 20026 solver.cpp:204]     Train net output #0: loss = 0.539237 (* 1 = 0.539237 loss)
I0111 13:40:11.137861 20026 solver.cpp:470] Iteration 16400, lr = 0.0001
I0111 13:40:19.651949 20026 solver.cpp:266] Iteration 16500, Testing net (#0)
I0111 13:40:21.541574 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6582
I0111 13:40:21.541599 20026 solver.cpp:315]     Test net output #1: loss = 1.0466 (* 1 = 1.0466 loss)
I0111 13:40:21.563874 20026 solver.cpp:189] Iteration 16500, loss = 0.69816
I0111 13:40:21.563894 20026 solver.cpp:204]     Train net output #0: loss = 0.69816 (* 1 = 0.69816 loss)
I0111 13:40:21.563899 20026 solver.cpp:470] Iteration 16500, lr = 0.0001
I0111 13:40:30.146093 20026 solver.cpp:189] Iteration 16600, loss = 0.661592
I0111 13:40:30.146119 20026 solver.cpp:204]     Train net output #0: loss = 0.661592 (* 1 = 0.661592 loss)
I0111 13:40:30.146123 20026 solver.cpp:470] Iteration 16600, lr = 0.0001
I0111 13:40:38.740260 20026 solver.cpp:189] Iteration 16700, loss = 0.791507
I0111 13:40:38.740285 20026 solver.cpp:204]     Train net output #0: loss = 0.791507 (* 1 = 0.791507 loss)
I0111 13:40:38.740290 20026 solver.cpp:470] Iteration 16700, lr = 0.0001
I0111 13:40:47.324359 20026 solver.cpp:189] Iteration 16800, loss = 0.656894
I0111 13:40:47.324424 20026 solver.cpp:204]     Train net output #0: loss = 0.656894 (* 1 = 0.656894 loss)
I0111 13:40:47.324429 20026 solver.cpp:470] Iteration 16800, lr = 0.0001
I0111 13:40:55.924368 20026 solver.cpp:189] Iteration 16900, loss = 0.539678
I0111 13:40:55.924393 20026 solver.cpp:204]     Train net output #0: loss = 0.539678 (* 1 = 0.539678 loss)
I0111 13:40:55.924398 20026 solver.cpp:470] Iteration 16900, lr = 0.0001
I0111 13:41:04.506650 20026 solver.cpp:334] Snapshotting to snapshots/snapshot_iter_17000.caffemodel
I0111 13:41:04.538540 20026 solver.cpp:342] Snapshotting solver state to snapshots/snapshot_iter_17000.solverstate
I0111 13:41:04.558601 20026 solver.cpp:266] Iteration 17000, Testing net (#0)
I0111 13:41:06.405102 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6594
I0111 13:41:06.405127 20026 solver.cpp:315]     Test net output #1: loss = 1.0411 (* 1 = 1.0411 loss)
I0111 13:41:06.427196 20026 solver.cpp:189] Iteration 17000, loss = 0.690666
I0111 13:41:06.427217 20026 solver.cpp:204]     Train net output #0: loss = 0.690666 (* 1 = 0.690666 loss)
I0111 13:41:06.427222 20026 solver.cpp:470] Iteration 17000, lr = 0.0001
I0111 13:41:15.020164 20026 solver.cpp:189] Iteration 17100, loss = 0.655516
I0111 13:41:15.020195 20026 solver.cpp:204]     Train net output #0: loss = 0.655516 (* 1 = 0.655516 loss)
I0111 13:41:15.020200 20026 solver.cpp:470] Iteration 17100, lr = 0.0001
I0111 13:41:23.605453 20026 solver.cpp:189] Iteration 17200, loss = 0.768568
I0111 13:41:23.605537 20026 solver.cpp:204]     Train net output #0: loss = 0.768568 (* 1 = 0.768568 loss)
I0111 13:41:23.605543 20026 solver.cpp:470] Iteration 17200, lr = 0.0001
I0111 13:41:32.207854 20026 solver.cpp:189] Iteration 17300, loss = 0.645736
I0111 13:41:32.207880 20026 solver.cpp:204]     Train net output #0: loss = 0.645736 (* 1 = 0.645736 loss)
I0111 13:41:32.207885 20026 solver.cpp:470] Iteration 17300, lr = 0.0001
I0111 13:41:40.854089 20026 solver.cpp:189] Iteration 17400, loss = 0.531709
I0111 13:41:40.854113 20026 solver.cpp:204]     Train net output #0: loss = 0.531709 (* 1 = 0.531709 loss)
I0111 13:41:40.854117 20026 solver.cpp:470] Iteration 17400, lr = 0.0001
I0111 13:41:49.377374 20026 solver.cpp:266] Iteration 17500, Testing net (#0)
I0111 13:41:51.268610 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6605
I0111 13:41:51.268632 20026 solver.cpp:315]     Test net output #1: loss = 1.0378 (* 1 = 1.0378 loss)
I0111 13:41:51.291107 20026 solver.cpp:189] Iteration 17500, loss = 0.686159
I0111 13:41:51.291126 20026 solver.cpp:204]     Train net output #0: loss = 0.686159 (* 1 = 0.686159 loss)
I0111 13:41:51.291131 20026 solver.cpp:470] Iteration 17500, lr = 0.0001
I0111 13:41:59.877015 20026 solver.cpp:189] Iteration 17600, loss = 0.652222
I0111 13:41:59.877079 20026 solver.cpp:204]     Train net output #0: loss = 0.652222 (* 1 = 0.652222 loss)
I0111 13:41:59.877084 20026 solver.cpp:470] Iteration 17600, lr = 0.0001
I0111 13:42:08.465585 20026 solver.cpp:189] Iteration 17700, loss = 0.761138
I0111 13:42:08.465611 20026 solver.cpp:204]     Train net output #0: loss = 0.761138 (* 1 = 0.761138 loss)
I0111 13:42:08.465615 20026 solver.cpp:470] Iteration 17700, lr = 0.0001
I0111 13:42:17.050407 20026 solver.cpp:189] Iteration 17800, loss = 0.641079
I0111 13:42:17.050434 20026 solver.cpp:204]     Train net output #0: loss = 0.641079 (* 1 = 0.641079 loss)
I0111 13:42:17.050439 20026 solver.cpp:470] Iteration 17800, lr = 0.0001
I0111 13:42:25.643054 20026 solver.cpp:189] Iteration 17900, loss = 0.527069
I0111 13:42:25.643079 20026 solver.cpp:204]     Train net output #0: loss = 0.527069 (* 1 = 0.527069 loss)
I0111 13:42:25.643084 20026 solver.cpp:470] Iteration 17900, lr = 0.0001
I0111 13:42:34.232935 20026 solver.cpp:334] Snapshotting to snapshots/snapshot_iter_18000.caffemodel
I0111 13:42:34.264646 20026 solver.cpp:342] Snapshotting solver state to snapshots/snapshot_iter_18000.solverstate
I0111 13:42:34.284626 20026 solver.cpp:266] Iteration 18000, Testing net (#0)
I0111 13:42:36.131374 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6611
I0111 13:42:36.131399 20026 solver.cpp:315]     Test net output #1: loss = 1.03751 (* 1 = 1.03751 loss)
I0111 13:42:36.153374 20026 solver.cpp:189] Iteration 18000, loss = 0.679357
I0111 13:42:36.153393 20026 solver.cpp:204]     Train net output #0: loss = 0.679357 (* 1 = 0.679357 loss)
I0111 13:42:36.153398 20026 solver.cpp:470] Iteration 18000, lr = 0.0001
I0111 13:42:44.738747 20026 solver.cpp:189] Iteration 18100, loss = 0.649118
I0111 13:42:44.738772 20026 solver.cpp:204]     Train net output #0: loss = 0.649118 (* 1 = 0.649118 loss)
I0111 13:42:44.738777 20026 solver.cpp:470] Iteration 18100, lr = 0.0001
I0111 13:42:53.324434 20026 solver.cpp:189] Iteration 18200, loss = 0.753362
I0111 13:42:53.324460 20026 solver.cpp:204]     Train net output #0: loss = 0.753362 (* 1 = 0.753362 loss)
I0111 13:42:53.324463 20026 solver.cpp:470] Iteration 18200, lr = 0.0001
I0111 13:43:01.911154 20026 solver.cpp:189] Iteration 18300, loss = 0.635336
I0111 13:43:01.911180 20026 solver.cpp:204]     Train net output #0: loss = 0.635336 (* 1 = 0.635336 loss)
I0111 13:43:01.911183 20026 solver.cpp:470] Iteration 18300, lr = 0.0001
I0111 13:43:10.497417 20026 solver.cpp:189] Iteration 18400, loss = 0.521944
I0111 13:43:10.497493 20026 solver.cpp:204]     Train net output #0: loss = 0.521944 (* 1 = 0.521944 loss)
I0111 13:43:10.497499 20026 solver.cpp:470] Iteration 18400, lr = 0.0001
I0111 13:43:18.999897 20026 solver.cpp:266] Iteration 18500, Testing net (#0)
I0111 13:43:20.896692 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6615
I0111 13:43:20.896720 20026 solver.cpp:315]     Test net output #1: loss = 1.03683 (* 1 = 1.03683 loss)
I0111 13:43:20.918941 20026 solver.cpp:189] Iteration 18500, loss = 0.671769
I0111 13:43:20.918962 20026 solver.cpp:204]     Train net output #0: loss = 0.671769 (* 1 = 0.671769 loss)
I0111 13:43:20.918967 20026 solver.cpp:470] Iteration 18500, lr = 0.0001
I0111 13:43:29.512259 20026 solver.cpp:189] Iteration 18600, loss = 0.648655
I0111 13:43:29.512284 20026 solver.cpp:204]     Train net output #0: loss = 0.648655 (* 1 = 0.648655 loss)
I0111 13:43:29.512290 20026 solver.cpp:470] Iteration 18600, lr = 0.0001
I0111 13:43:38.118347 20026 solver.cpp:189] Iteration 18700, loss = 0.7456
I0111 13:43:38.118373 20026 solver.cpp:204]     Train net output #0: loss = 0.7456 (* 1 = 0.7456 loss)
I0111 13:43:38.118378 20026 solver.cpp:470] Iteration 18700, lr = 0.0001
I0111 13:43:46.700279 20026 solver.cpp:189] Iteration 18800, loss = 0.62996
I0111 13:43:46.700345 20026 solver.cpp:204]     Train net output #0: loss = 0.62996 (* 1 = 0.62996 loss)
I0111 13:43:46.700351 20026 solver.cpp:470] Iteration 18800, lr = 0.0001
I0111 13:43:55.323941 20026 solver.cpp:189] Iteration 18900, loss = 0.518672
I0111 13:43:55.323971 20026 solver.cpp:204]     Train net output #0: loss = 0.518672 (* 1 = 0.518672 loss)
I0111 13:43:55.323977 20026 solver.cpp:470] Iteration 18900, lr = 0.0001
I0111 13:44:03.968406 20026 solver.cpp:334] Snapshotting to snapshots/snapshot_iter_19000.caffemodel
I0111 13:44:03.999701 20026 solver.cpp:342] Snapshotting solver state to snapshots/snapshot_iter_19000.solverstate
I0111 13:44:04.020028 20026 solver.cpp:266] Iteration 19000, Testing net (#0)
I0111 13:44:05.857748 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6612
I0111 13:44:05.857772 20026 solver.cpp:315]     Test net output #1: loss = 1.03674 (* 1 = 1.03674 loss)
I0111 13:44:05.879842 20026 solver.cpp:189] Iteration 19000, loss = 0.664707
I0111 13:44:05.879863 20026 solver.cpp:204]     Train net output #0: loss = 0.664707 (* 1 = 0.664707 loss)
I0111 13:44:05.879868 20026 solver.cpp:470] Iteration 19000, lr = 0.0001
I0111 13:44:14.493391 20026 solver.cpp:189] Iteration 19100, loss = 0.643727
I0111 13:44:14.493417 20026 solver.cpp:204]     Train net output #0: loss = 0.643727 (* 1 = 0.643727 loss)
I0111 13:44:14.493422 20026 solver.cpp:470] Iteration 19100, lr = 0.0001
I0111 13:44:23.333245 20026 solver.cpp:189] Iteration 19200, loss = 0.735923
I0111 13:44:23.333313 20026 solver.cpp:204]     Train net output #0: loss = 0.735923 (* 1 = 0.735923 loss)
I0111 13:44:23.333319 20026 solver.cpp:470] Iteration 19200, lr = 0.0001
I0111 13:44:32.262790 20026 solver.cpp:189] Iteration 19300, loss = 0.624595
I0111 13:44:32.262814 20026 solver.cpp:204]     Train net output #0: loss = 0.624595 (* 1 = 0.624595 loss)
I0111 13:44:32.262820 20026 solver.cpp:470] Iteration 19300, lr = 0.0001
I0111 13:44:40.950633 20026 solver.cpp:189] Iteration 19400, loss = 0.51259
I0111 13:44:40.950662 20026 solver.cpp:204]     Train net output #0: loss = 0.51259 (* 1 = 0.51259 loss)
I0111 13:44:40.950669 20026 solver.cpp:470] Iteration 19400, lr = 0.0001
I0111 13:44:49.607561 20026 solver.cpp:266] Iteration 19500, Testing net (#0)
I0111 13:44:51.497871 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6628
I0111 13:44:51.497895 20026 solver.cpp:315]     Test net output #1: loss = 1.03712 (* 1 = 1.03712 loss)
I0111 13:44:51.520161 20026 solver.cpp:189] Iteration 19500, loss = 0.661672
I0111 13:44:51.520180 20026 solver.cpp:204]     Train net output #0: loss = 0.661672 (* 1 = 0.661672 loss)
I0111 13:44:51.520185 20026 solver.cpp:470] Iteration 19500, lr = 0.0001
I0111 13:45:00.270824 20026 solver.cpp:189] Iteration 19600, loss = 0.639847
I0111 13:45:00.270894 20026 solver.cpp:204]     Train net output #0: loss = 0.639847 (* 1 = 0.639847 loss)
I0111 13:45:00.270900 20026 solver.cpp:470] Iteration 19600, lr = 0.0001
I0111 13:45:08.944234 20026 solver.cpp:189] Iteration 19700, loss = 0.729823
I0111 13:45:08.944258 20026 solver.cpp:204]     Train net output #0: loss = 0.729823 (* 1 = 0.729823 loss)
I0111 13:45:08.944263 20026 solver.cpp:470] Iteration 19700, lr = 0.0001
I0111 13:45:17.536414 20026 solver.cpp:189] Iteration 19800, loss = 0.621709
I0111 13:45:17.536439 20026 solver.cpp:204]     Train net output #0: loss = 0.621709 (* 1 = 0.621709 loss)
I0111 13:45:17.536443 20026 solver.cpp:470] Iteration 19800, lr = 0.0001
I0111 13:45:26.204980 20026 solver.cpp:189] Iteration 19900, loss = 0.502745
I0111 13:45:26.205006 20026 solver.cpp:204]     Train net output #0: loss = 0.502745 (* 1 = 0.502745 loss)
I0111 13:45:26.205011 20026 solver.cpp:470] Iteration 19900, lr = 0.0001
I0111 13:45:34.779264 20026 solver.cpp:334] Snapshotting to snapshots/snapshot_iter_20000.caffemodel
I0111 13:45:34.817075 20026 solver.cpp:342] Snapshotting solver state to snapshots/snapshot_iter_20000.solverstate
I0111 13:45:34.858968 20026 solver.cpp:248] Iteration 20000, loss = 0.655291
I0111 13:45:34.858989 20026 solver.cpp:266] Iteration 20000, Testing net (#0)
I0111 13:45:36.685187 20026 solver.cpp:315]     Test net output #0: accuracy = 0.6618
I0111 13:45:36.685211 20026 solver.cpp:315]     Test net output #1: loss = 1.03722 (* 1 = 1.03722 loss)
I0111 13:45:36.685216 20026 solver.cpp:253] Optimization Done.
I0111 13:45:36.685220 20026 caffe.cpp:121] Optimization Done.
